{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d82179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, Subset\n",
    "\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import AerSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b43ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadamardTransform:\n",
    "    def __init__(self):\n",
    "        self.backend = AerSimulator(method=\"statevector\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # ensures cpu usage\n",
    "        if x.is_cuda:\n",
    "            x = x.cpu()\n",
    "\n",
    "        # flatten image for qiskit\n",
    "        img_flat = x.flatten().detach().numpy().astype(np.float64)\n",
    "        len = img_flat.size\n",
    "\n",
    "        # find number of qubits\n",
    "        n_qubits = int(np.ceil(np.log2(len)))\n",
    "        N = np.pow(2, n_qubits)\n",
    "\n",
    "        # pad\n",
    "        state_vector = np.zeros(N, dtype=np.float64)\n",
    "        state_vector[:len] = img_flat\n",
    "\n",
    "        # normalize constant\n",
    "        norm = np.linalg.norm(state_vector)\n",
    "\n",
    "        # cover edge case of fully black image\n",
    "        if norm == 0:\n",
    "            return torch.zeros_like(x)\n",
    "        \n",
    "        # normalize the state vector\n",
    "        state_vector = state_vector / norm\n",
    "\n",
    "        # circuit initialized with flattened image and hadamard on all rows\n",
    "        qc = QuantumCircuit(n_qubits)\n",
    "        qc.initialize(state_vector, qc.qubits)\n",
    "        qc.h(range(n_qubits))\n",
    "        qc.save_statevector()\n",
    "\n",
    "        # print(state_vector)\n",
    "\n",
    "        # transpile creates simulatible circuit\n",
    "        tqc = transpile(qc, self.backend)\n",
    "        result = self.backend.run(tqc).result()\n",
    "        state = np.asarray(result.get_statevector(tqc))\n",
    "\n",
    "        # reshape and get real amplitudes\n",
    "        y = np.real(state[:len]) * norm\n",
    "        y = y.reshape(x.shape).astype(np.float32)\n",
    "\n",
    "        return torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3382893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download format\n",
    "# turns MNIST images to PyTorch tensors and normalizes between [-1,1] centered at 0\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    HadamardTransform()\n",
    "])\n",
    "\n",
    "# download data to computer\n",
    "raw_train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "raw_test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,            # testing data so labels are unknown during training\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1e7ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ht to all images before training so that i dont run qiskit every epoch\n",
    "# ht qiskit class\n",
    "ht = HadamardTransform()\n",
    "\n",
    "def precompute(dataset):\n",
    "\n",
    "    temp_image_list, temp_label_list = [], []\n",
    "\n",
    "    for image, label in DataLoader(dataset, batch_size=1, shuffle=False):\n",
    "        image_ht = ht(image[0])                         # get rid of batch dimensions so i can apply the ht\n",
    "        temp_image_list.append(image_ht.unsqueeze(0))   # undo previous step and append to temp_image_list\n",
    "        temp_label_list.append(label)\n",
    "\n",
    "    concatinated_image_dataset = torch.cat(temp_image_list, dim=0) # combines all tensors in temp lists to a single tensor with batch dimensions\n",
    "    concatinated_label_dataset = torch.cat(temp_label_list, dim=0)\n",
    "    return TensorDataset(concatinated_image_dataset, concatinated_label_dataset)\n",
    "\n",
    "# sanity test lines to make sure the cnn works\n",
    "# it works but it takes a super long time to run all images in the fashion mnist\n",
    "raw_train_small = Subset(raw_train_dataset, range(500))\n",
    "raw_test_small = Subset(raw_test_dataset, range(100))\n",
    "\n",
    "# use raw_train_small and raw_test_small to decrease number of training and test images\n",
    "# use raw_train_dataset and raw_test_dataset for entire dataset\n",
    "train_dataset = precompute(raw_train_small)\n",
    "test_dataset  = precompute(raw_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f0eaf35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,         # each epoch is 128 samples\n",
    "    shuffle=True            # randomize after each training epoch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=256,         # each epoch is 256 samples\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# test shapes of pytorch datasets\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a21ce0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# create model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c4674d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 2.0499\n",
      "Epoch [2/3], Loss: 1.3391\n",
      "Epoch [3/3], Loss: 0.9611\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()        # reset gradients\n",
    "        outputs = model(images)      # forward\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()              # backward\n",
    "        optimizer.step()             # update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6188f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 68.00%\n"
     ]
    }
   ],
   "source": [
    "# evalulate results and accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
